{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset Similarity\n",
    "\n",
    "The analyzing the similar datasets offers several use case for our data management purposes. Such as dataset recommendation and data integration or dataset merging. There are four dimensions of similarity between datasets which are be considered:\n",
    "\n",
    "### 1.\tSpatial Similarity:\n",
    "The spatial similarity between two datasets is determined by calculating the geospatial area they cover. As is commonly observed, the longitude and latitude values for the same location may not match exactly when recorded from different sources. To account for this, hexagonal grids (with a 20-kilometer side length) are created. Points (longitude and latitude) that fall within the same grid are considered to cover the same area. The spatial similarity between Dataset #A and Dataset #B is expressed as the percentage of grids covered by Dataset #A, divided by the total number of grids covered by Dataset #B. Thus, this score is asymmetric in nature.\n",
    "\n",
    "### 2.\tSpatial Similarity:\n",
    "The spatial similarity between two datasets is determined by calculating the geospatial area they cover. As is commonly observed, the longitude and latitude values for the same location may not match exactly when recorded from different sources. To account for this, hexagonal grids (with a 20-kilometer side length) are created. Points (longitude and latitude) that fall within the same grid are considered to cover the same area. The spatial similarity between Dataset #A and Dataset #B is expressed as the percentage of grids covered by Dataset #A, divided by the total number of grids covered by Dataset #B. Thus, this score is asymmetric in nature.\n",
    "\n",
    "### 3.\tAttribute Similarity:\n",
    "The attributes of dataset signifies the type of dataset either capture biochemical or ocean variables. Similar attributes between two datasets may interest user to look into similar data. The attribute similarity is calculated as percentage of same attributes in Dataset#A with respect to Dataset#B. Thus, this score is asymmetric in nature.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Step#1: Downloading Metadata from Erddap\n",
    "\n",
    "This step will traverse all the data available on CIOOS Altantic and saves `alldataset_das_dict_NEW.pkl`\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43d1156ad129fca2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "from requests_html import HTMLSession\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    " # base_url = \"https://cioosatlantic.ca/erddap/tabledap/index.html?page=1&itemsPerPage=1000\"\n",
    "base_url  = \"https://catalogue.cioosatlantic.ca/api/3/action/resource_search?query=name:erddap\"\n",
    "\n",
    "def parse_json(json_object_):\n",
    "    ds_profile_dict = {}\n",
    "    lst_of_rows_ = json_object_['table']['rows']\n",
    "\n",
    "    last_ = None\n",
    "    for rtype, v_name, att_name, dtype, val_  in lst_of_rows_:\n",
    "        if rtype == \"variable\":\n",
    "            if v_name not in ds_profile_dict:\n",
    "                ds_profile_dict[v_name] = {\"dtype\": dtype, \"attr\": {}}\n",
    "            last_ = (rtype, v_name, att_name, dtype, val_)\n",
    "        elif rtype == 'attribute':\n",
    "            if v_name not in ds_profile_dict:\n",
    "                ds_profile_dict[v_name] = {}\n",
    "            else:\n",
    "                if (last_ is not None) and last_[0] == 'variable':\n",
    "                    assert att_name not in ds_profile_dict[v_name][\"attr\"], \"error unknown\"\n",
    "                    ds_profile_dict[v_name][\"attr\"][att_name] = ( dtype, val_)\n",
    "                else:\n",
    "                    ds_profile_dict[v_name][att_name] = (dtype, val_)\n",
    "        else:\n",
    "            print(\"=====> Row type Error (Not found)\")\n",
    "            exit(-1)\n",
    "\n",
    "    return ds_profile_dict\n",
    "\n",
    "session = HTMLSession()\n",
    "# Send an HTTP GET request to the URL\n",
    "response = session.get(base_url)\n",
    "# Render JavaScript if needed (optional)\n",
    "# response.html.render()\n",
    "json_list_links = json.loads(response.text)\n",
    "all_elements = [r_id['url'] for r_id in json_list_links['result']['results']]\n",
    "list_of_dictionaries_ = []\n",
    "for elem in tqdm(all_elements):\n",
    "    new_requ_ = elem\n",
    "    if not new_requ_.endswith(\".html\"):\n",
    "        new_requ_ = (new_requ_+\".html\")\n",
    "    if \"/erddap/info\" not in new_requ_:\n",
    "        new_requ_ = new_requ_.replace(\"/erddap/tabledap\", \"/erddap/info\")\n",
    "    if \"index.html\" in new_requ_:\n",
    "        new_requ_ = new_requ_.replace(\"index.html\", \"index.json\")\n",
    "    if \".html\" in new_requ_:\n",
    "        new_requ_ = new_requ_.replace(\".html\", \"/index.json\")\n",
    "    das_response = session.get(new_requ_)\n",
    "    if das_response.reason == '404':\n",
    "        print(f\"URL not found: {elem}\")\n",
    "        continue\n",
    "    json_obj_ = das_response.json()\n",
    "    ds_profile_dictionary_ = parse_json(json_obj_)\n",
    "    list_of_dictionaries_.append(ds_profile_dictionary_)\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"Number of das files: {list_of_dictionaries_.__len__()}\")\n",
    "pickle.dump(list_of_dictionaries_, open(\"./alldataset_das_dict_NEW.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step#2: \n",
    "\n",
    "Matching of overlapping dataset having same attributes\n",
    "\n",
    "Input: 'alldataset_das_dict_NEW.pkl' file generated by '__download_erddap_metadata.py'\n",
    "Output: 'df_attributal_matching_score.csv' \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1ab7a48a7c12591"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# run __download_erddap_metadata.py\n",
    "erdap_dict_ = pickle.load(open(\"../res/alldataset_das_dict_NEW.pkl\", 'rb'))\n",
    "\n",
    "list_of_das_variables_ = []\n",
    "item_labels = []\n",
    "for dset_ in erdap_dict_:\n",
    "    item_labels.append(dset_['NC_GLOBAL']['title'][1])\n",
    "    del [dset_['NC_GLOBAL']]\n",
    "    list_of_das_variables_.append(set(dset_.keys()))\n",
    "\n",
    "num_items =  len(list_of_das_variables_)\n",
    "# Initialize a 50x50 matrix with zeros\n",
    "matching_matrix = np.zeros((num_items, num_items), dtype=int)\n",
    "\n",
    "lod = list_of_das_variables_\n",
    "# Calculate matching scores\n",
    "for i in range(num_items):\n",
    "    for j in range(num_items):\n",
    "        total_ = len(lod[i])\n",
    "        match_count = len(lod[i].intersection(lod[j]))\n",
    "        match_count = int(float(match_count/total_)*100)\n",
    "\n",
    "\n",
    "        matching_matrix[i][j] = match_count\n",
    "\n",
    "\n",
    "df_matching = pd.DataFrame(matching_matrix, index=item_labels, columns=item_labels)\n",
    "df_matching.to_csv(\"./res/df_attributal_matching_score.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5081981d9374253"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step#3:\n",
    "Calculating spatial matching score between datasets. the spatial window of 20KM is set.\n",
    "\n",
    "Input: 'grid-ocean-lakes-20KM-EPSG-3857.shp' contains lakes map, generated from QGIS\n",
    "\n",
    "Output: '_df_spatial_matching_score.csv' \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8866fa3458eb196d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pickle\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# this .shp file is created from QGIS which contains map of all lakes\n",
    "grid_filename = \"/res/grid-ocean-lakes-20KM-EPSG-3857.shp\"\n",
    "gdf_filename = grid_filename.replace(\".shp\",\"_gdf.pkl\")\n",
    "ckdtree_filename = grid_filename.replace(\".shp\",\"_ckdtree.pkl\")\n",
    "dataset_grid_dict_filename = grid_filename.replace(\".shp\", \"_das_gridcenter_dict.pkl\")\n",
    "\n",
    "if not os.path.exists(gdf_filename):\n",
    "    gdf_ = gpd.read_file(grid_filename)\n",
    "    gdf_['centroid'] = gdf_.geometry.centroid\n",
    "    target_crs = 'EPSG:4269'\n",
    "    centroids_gdf = gdf_[['centroid']].copy()\n",
    "    centroids_gdf.set_geometry('centroid', inplace=True)\n",
    "    gdf_['centroid'] = centroids_gdf.to_crs(target_crs)\n",
    "    gdf_['cen_x'] = gdf_['centroid'].x\n",
    "    gdf_['cen_y'] = gdf_['centroid'].y\n",
    "\n",
    "    nB = np.array(list(gdf_['centroid'].geometry.apply(lambda x: (x.x, x.y))))\n",
    "    btree = cKDTree(nB)\n",
    "    pickle.dump(btree, open(ckdtree_filename, 'wb'))\n",
    "    pickle.dump(gdf_, open(gdf_filename, 'wb'))\n",
    "else:\n",
    "    print(\"Loading GDF and \")\n",
    "    gdf_ = pickle.load(open(gdf_filename, 'rb'))\n",
    "    btree = pickle.load(open(ckdtree_filename, 'rb'))\n",
    "\n",
    "dataset_grids_map = {} #dataset_idx = [ grid_centroids,.. ]\n",
    "if not os.path.exists(dataset_grid_dict_filename):\n",
    "    for root, dir, files in os.walk(\"../erddap data/\"):\n",
    "        pbar_ = tqdm(files)\n",
    "        for fname in pbar_:\n",
    "            pbar_.set_description(fname)\n",
    "            if fname.lower().endswith(\".csv\"):\n",
    "                fl_path_ = os.path.join(root, fname)\n",
    "                df = pd.read_csv(fl_path_).dropna()\n",
    "                if 'latitude' not in df.columns:\n",
    "                    print(f\"Skipping [{fname}]\")\n",
    "                    continue\n",
    "\n",
    "                points = set(zip(df['latitude'].iloc[1:].astype('float64'), df['longitude'].iloc[1:].astype('float64')))\n",
    "                points_ndarray = np.array(list(points))\n",
    "                dist, idx = btree.query(points_ndarray, k=1)\n",
    "                new_points = btree.data[idx]\n",
    "                new_points_df_ = pd.DataFrame(new_points, columns=['cen_x', 'cen_y'])\n",
    "                merged_df = gdf_.merge(new_points_df_, on=['cen_x', 'cen_y'], how='inner')\n",
    "                matched_ids = set([int(x) for x in merged_df['id'].tolist()])\n",
    "                dataset_grids_map[fname] = matched_ids\n",
    "\n",
    "    pickle.dump(dataset_grids_map, open(dataset_grid_dict_filename, 'wb'))\n",
    "\n",
    "else:\n",
    "    dataset_grids_map = pickle.load(open(dataset_grid_dict_filename, 'rb'))\n",
    "\n",
    "\n",
    "list_of_das_variables_ = []\n",
    "item_labels = []\n",
    "for d_name, grids_ in dataset_grids_map.items():\n",
    "    item_labels.append(d_name)\n",
    "    list_of_das_variables_.append(grids_)\n",
    "\n",
    "num_items = len(list_of_das_variables_)\n",
    "# Initialize a 50x50 matrix with zeros\n",
    "matching_matrix = np.zeros((num_items, num_items), dtype=int)\n",
    "\n",
    "lod = list_of_das_variables_\n",
    "# Calculate matching scores\n",
    "for i in tqdm(range(num_items)):\n",
    "    for j in range(num_items):\n",
    "        total_ = len(lod[i])\n",
    "        match_count = len(set(lod[i]).intersection(set(lod[j])))\n",
    "        match_count = int(float(match_count / total_) * 100)\n",
    "        # if match_count != 100:\n",
    "        #     match_count = 0\n",
    "        matching_matrix[i][j] = match_count\n",
    "\n",
    "df_matching = pd.DataFrame(matching_matrix, index=item_labels, columns=item_labels)\n",
    "df_matching.to_csv(f\"./res/{os.path.basename(grid_filename).replace('.shp','_df_spatial_matching_score.csv')}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b0c5a4c0132aefd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step#4:\n",
    "\n",
    "Input: The data from erddap should been downloaded in the directory 'erddap data'.  Please refer to the Building Bridges directory in Google drive to copy the pre-downloaded datasets. \n",
    "\n",
    "This file traverse the timestamp of all datasets and calculate the overlapping dataset similarity\n",
    "the window size of 24 hours has been set to check overlap\n",
    "Output: date_df_temporal_matching_score"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c7569d6588c649e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_date_map = {}\n",
    "for root, dir, files in os.walk(\"../erddap data/\"):\n",
    "    pbar_ = tqdm(files)\n",
    "    for fname in pbar_:\n",
    "        pbar_.set_description(fname)\n",
    "        if fname.lower().endswith(\".csv\"):\n",
    "            fl_path_ = os.path.join(root, fname)\n",
    "            df = pd.read_csv(fl_path_).dropna()\n",
    "            if 'latitude' not in df.columns:\n",
    "                print(f\"Skipping [{fname}]\")\n",
    "                continue\n",
    "\n",
    "            s_ = pd.to_datetime(df['time'].iloc[1:], format='%Y-%m-%dT%H:%M:%SZ').dt.strftime('%Y%m%d').astype(int)\n",
    "            dataset_date_map[fname] = s_.unique()\n",
    "\n",
    "list_of_das_variables_ = []\n",
    "item_labels = []\n",
    "for d_name, dates_list in dataset_date_map.items():\n",
    "    item_labels.append(d_name)\n",
    "    list_of_das_variables_.append(dates_list)\n",
    "\n",
    "num_items = len(list_of_das_variables_)\n",
    "# Initialize a 50x50 matrix with zeros\n",
    "matching_matrix = np.zeros((num_items, num_items), dtype=int)\n",
    "\n",
    "lod = list_of_das_variables_\n",
    "# Calculate matching scores\n",
    "for i in tqdm(range(num_items)):\n",
    "    for j in range(num_items):\n",
    "        total_ = len(lod[i])\n",
    "        match_count = len(set(lod[i]).intersection(set(lod[j])))\n",
    "        match_count = int(float(match_count / total_) * 100)\n",
    "\n",
    "        matching_matrix[i][j] = match_count\n",
    "\n",
    "df_matching = pd.DataFrame(matching_matrix, index=item_labels, columns=item_labels)\n",
    "df_matching.to_csv(f\"./res/date_df_temporal_matching_score.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30f84865fd9d3fe8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step#5:\n",
    "\n",
    "Plotting \n",
    "\n",
    "![heatmap](res/screenshot_heatmap.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f71105c473266344"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/html": "<iframe\n    scrolling=\"no\"\n    width=\"1820px\"\n    height=\"1820\"\n    src=\"iframe_figures/figure_13.html\"\n    frameborder=\"0\"\n    allowfullscreen\n></iframe>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dash\n",
    "from dash import dcc, html, Input, Output\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe'              # Good fallback for PyCharm\n",
    "\n",
    "\n",
    "attr_score_ = pd.read_csv(\"../res/df_attributal_matching_score.csv\", index_col=0)\n",
    "spatial_score_ = pd.read_csv(\"../res/grid-ocean-lakes-20KM-EPSG-3857_df_spatial_matching_score.csv\", index_col=0)\n",
    "temporal_score_ = pd.read_csv(\"../res/date_df_temporal_matching_score.csv\", index_col=0)\n",
    "\n",
    "# Clean/rename indices to match\n",
    "new_spatial_score_ = pd.DataFrame(columns=attr_score_.columns)\n",
    "col_row_to_remove = []\n",
    "rename_columns_ = {}\n",
    "for i, row in attr_score_.iterrows():\n",
    "    up_i = i.replace(\",\", \" \").replace(\":\", \"\").replace(\"/\", \" \").replace(\"|\", \"\") + \".csv\"\n",
    "    if up_i not in spatial_score_.index:\n",
    "        col_row_to_remove.append(i)\n",
    "    else:\n",
    "        rename_columns_[i] = up_i\n",
    "attr_score_.drop(columns=col_row_to_remove, inplace=True)\n",
    "attr_score_.drop(col_row_to_remove, inplace=True)\n",
    "attr_score_.rename(columns=rename_columns_, inplace=True)\n",
    "attr_score_.rename(index=rename_columns_, inplace=True)\n",
    "\n",
    "# Set thresholds manually\n",
    "attr_threshold = 0\n",
    "spatial_threshold = 0\n",
    "temporal_threshold_ = 40\n",
    "default_value = -1\n",
    "\n",
    "# Apply threshold masks\n",
    "mask1 = attr_score_ >= attr_threshold\n",
    "mask2 = spatial_score_ >= spatial_threshold\n",
    "mask3 = temporal_score_ >= temporal_threshold_\n",
    "combined_mask = mask1 & mask2 & mask3\n",
    "\n",
    "attr_score_df_ = attr_score_.where(combined_mask, default_value)\n",
    "spatial_score_df_ = spatial_score_.where(combined_mask, default_value)\n",
    "temporal_score_df_ = temporal_score_.where(combined_mask, default_value)\n",
    "\n",
    "# Plot using plotly\n",
    "colors = ['Greens', 'Blues', 'Oranges']\n",
    "\n",
    "heatmap1 = go.Heatmap(\n",
    "    z=attr_score_df_.values,\n",
    "    x=list(attr_score_df_.columns),\n",
    "    y=list(attr_score_df_.index),\n",
    "    opacity=0.2,\n",
    "    colorscale=colors[0],\n",
    "    zmax=150,\n",
    "    zmin=0,\n",
    "    colorbar=dict(title='Attribute', orientation='h', len=0.33, x=0.1),\n",
    "    showscale=True,\n",
    "    customdata=np.dstack((spatial_score_df_, attr_score_df_, temporal_score_df_)),\n",
    "    hovertemplate=(\n",
    "        \"Row: %{y}<br>\"\n",
    "        \"Col: %{x}<br>\"\n",
    "        \"Spatial: %{customdata[0]}<br>\"\n",
    "        \"Temp: %{customdata[2]}<br>\"\n",
    "        \"Attr: %{z}<br>\"\n",
    "        \"<extra></extra>\"\n",
    "    )\n",
    ")\n",
    "\n",
    "heatmap2 = go.Heatmap(\n",
    "    z=spatial_score_df_.values,\n",
    "    x=list(spatial_score_df_.columns),\n",
    "    y=list(spatial_score_df_.index),\n",
    "    colorscale=colors[1],\n",
    "    opacity=0.2,\n",
    "    zmax=150,\n",
    "    zmin=0,\n",
    "    colorbar=dict(title='Spatial', orientation='h', len=0.33, x=0.8),\n",
    "    showscale=True\n",
    ")\n",
    "\n",
    "heatmap3 = go.Heatmap(\n",
    "    z=temporal_score_df_.values,\n",
    "    x=list(temporal_score_df_.columns),\n",
    "    y=list(temporal_score_df_.index),\n",
    "    opacity=0.2,\n",
    "    zmax=150,\n",
    "    zmin=0,\n",
    "    colorscale=colors[2],\n",
    "    colorbar=dict(title='Temporal', orientation='h', len=0.33, x=0.46),\n",
    "    showscale=True\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[heatmap3, heatmap2, heatmap1])\n",
    "fig.update_layout(\n",
    "    width=1800,\n",
    "    height=1800,\n",
    "    xaxis=dict(tickangle=90, tickfont=dict(size=4)),\n",
    "    yaxis=dict(tickfont=dict(size=4)),\n",
    "    title=dict(font=dict(size=20)),\n",
    "    font=dict(size=10)\n",
    ")\n",
    "\n",
    "# Show the figure in notebook\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-23T16:48:07.030606Z",
     "start_time": "2025-07-23T16:48:06.862175600Z"
    }
   },
   "id": "b0763435ad6e965e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Problem Visualizing in Jupyter Notebook\n",
    "\n",
    "If you are getting problem visualizing in notebook, run code below in .py file "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1c152a4466eaeb9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "attr_score_ = pd.read_csv(\"../res/df_attributal_matching_score.csv\", index_col=0)\n",
    "spatial_score_ = pd.read_csv(\"../res/grid-ocean-lakes-20KM-EPSG-3857_df_spatial_matching_score.csv\", index_col=0)\n",
    "temporal_score_ = pd.read_csv(\"../res/date_df_temporal_matching_score.csv\", index_col=0)\n",
    "\n",
    "new_spatial_score_ = pd.DataFrame(columns=attr_score_.columns)\n",
    "col_row_to_remove = []\n",
    "rename_columns_ = {}\n",
    "for i, row in attr_score_.iterrows():\n",
    "    up_i = i.replace(\",\", \" \").replace(\":\", \"\").replace(\"/\", \" \").replace(\"|\", \"\")\n",
    "    up_i = up_i + \".csv\"\n",
    "    if up_i not in spatial_score_.index:\n",
    "        col_row_to_remove.append(i)\n",
    "    else:\n",
    "        rename_columns_[i] = up_i\n",
    "\n",
    "attr_score_.drop(columns=col_row_to_remove, inplace=True)\n",
    "attr_score_.drop(col_row_to_remove, inplace=True)\n",
    "attr_score_.rename(columns=rename_columns_, inplace=True)\n",
    "attr_score_.rename(index=rename_columns_, inplace=True)\n",
    "\n",
    "\n",
    "attr_min__ = 0\n",
    "attr_max__ = max(attr_score_.max())\n",
    "spatial_min__ = 0\n",
    "spatial_max__ = max(spatial_score_.max())\n",
    "temporal_min__ = 0\n",
    "temporal_max__ = min(temporal_score_.max())\n",
    "\n",
    "\n",
    "# Initialize the Dash app\n",
    "app = dash.Dash(__name__)\n",
    "app.title = \"Interactive Heatmap with Thresholds\"\n",
    "\n",
    "# Define the layout\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Interactive Heatmap with Threshold Filters\", style={'textAlign': 'center'}),\n",
    "\n",
    "    html.Div([\n",
    "        html.Div([\n",
    "            html.Label(\"Temporal Threshold:\", style={'font-weight': 'bold'}),\n",
    "            dcc.Slider(\n",
    "                id='temporal-threshold',\n",
    "                min=temporal_min__,\n",
    "                max=temporal_max__,\n",
    "                step=1,\n",
    "                value=40,  # Default value\n",
    "                marks={i: str(i) for i in range(0, 101, 10)},\n",
    "                tooltip={\"placement\": \"bottom\", \"always_visible\": True},\n",
    "            ),\n",
    "        ], style={'width': '45%', 'display': 'inline-block', 'padding': '0 20'}),\n",
    "\n",
    "        html.Div([\n",
    "            html.Label(\"Attributes Threshold:\", style={'font-weight': 'bold'}),\n",
    "            dcc.Slider(\n",
    "                id='attr-threshold',\n",
    "                min=attr_min__,\n",
    "                max=attr_max__,\n",
    "                step=1,\n",
    "                value=0,  # Default value\n",
    "                marks={i: str(i) for i in range(0, 101, 10)},\n",
    "                tooltip={\"placement\": \"bottom\", \"always_visible\": True},\n",
    "            ),\n",
    "        ], style={'width': '45%', 'display': 'inline-block', 'padding': '0 20'}),\n",
    "\n",
    "        html.Div([\n",
    "            html.Label(\"Spatial Threshold:\", style={'font-weight': 'bold'}),\n",
    "            dcc.Slider(\n",
    "                id='spatial-threshold',\n",
    "                min=spatial_min__,\n",
    "                max=spatial_max__,\n",
    "                step=1,\n",
    "                value=0,  # Default value\n",
    "                marks={i: str(i) for i in range(0, 101, 10)},\n",
    "                tooltip={\"placement\": \"bottom\", \"always_visible\": True},\n",
    "            ),\n",
    "        ], style={'width': '45%', 'display': 'inline-block', 'padding': '0 20'}),\n",
    "    ], style={'padding': '40px'}),\n",
    "\n",
    "    dcc.Graph(id='heatmap')\n",
    "])\n",
    "\n",
    "\n",
    "# Define the callback to update the heatmap\n",
    "@app.callback(\n",
    "    Output('heatmap', 'figure'),\n",
    "    [Input('attr-threshold', 'value'),\n",
    "     Input('spatial-threshold', 'value'),\n",
    "     Input('temporal-threshold', 'value')]\n",
    ")\n",
    "\n",
    "\n",
    "def update_heatmap(attr_threshold, spatial_threshold, temporal_threshold_):\n",
    "    # Create a mask based on the thresholds\n",
    "    attr_score_df_, spatial_score_df_, temporal_score_df_ = attr_score_.copy(), spatial_score_.copy(), temporal_score_.copy()\n",
    "    mask1 = attr_score_df_ >= attr_threshold\n",
    "    mask2 = spatial_score_df_ >= spatial_threshold\n",
    "    mask3 = temporal_score_df_ >= temporal_threshold_\n",
    "    # print(f\"Attr: {attr_threshold}  - Spatial: {spatial_threshold} - Temp: {temporal_threshold_}\")\n",
    "    combined_mask = mask1 & mask2 & mask3\n",
    "    default_value  = -1\n",
    "    attr_score_df_.where(combined_mask, default_value, inplace=True)\n",
    "    spatial_score_df_.where(combined_mask, default_value,  inplace=True)\n",
    "    temporal_score_df_.where(combined_mask, default_value, inplace=True)\n",
    "\n",
    "    #validation\n",
    "    flag_ = False\n",
    "    for col in attr_score_df_.columns:\n",
    "        for idx in attr_score_df_.index.values:\n",
    "            if (attr_score_df_.loc[idx][col] != default_value) and (attr_score_df_.loc[idx][col] < attr_threshold):\n",
    "                flag_ = True\n",
    "                print(f\"ATTR [{attr_score_df_.loc[idx][col]}  < {attr_threshold}]\")\n",
    "                break\n",
    "            if (spatial_score_df_.loc[idx][col] != default_value) and (spatial_score_df_.loc[idx][col] < spatial_threshold):\n",
    "                print(f\"SPAT [{spatial_score_df_.loc[idx][col]}  < {spatial_threshold}]\")\n",
    "                flag_ = True\n",
    "                break\n",
    "            if (temporal_score_df_.loc[idx][col] != default_value) and (temporal_score_df_.loc[idx][col] < temporal_threshold_):\n",
    "                print(f\"TEMP [{temporal_score_df_.loc[idx][col]} < {temporal_threshold_}]\")\n",
    "                flag_ = True\n",
    "                break\n",
    "        if flag_:\n",
    "            break\n",
    "    if flag_:\n",
    "        print(\"Something is wrong.....!\")\n",
    "\n",
    "\n",
    "    colors = ['Greens', 'Blues', 'Oranges']\n",
    "    # colors = ['Purp', 'Oryel','Reds']\n",
    "    # Create the heatmap\n",
    "    heatmap1 = go.Heatmap(\n",
    "        z=attr_score_df_.values,\n",
    "        x=list(attr_score_df_.columns),\n",
    "        y=list(attr_score_df_.index),\n",
    "        opacity=0.2,\n",
    "        colorscale=colors[0],\n",
    "        zmax=150, zauto=False, zmin=0,\n",
    "        colorbar=dict(title='Attribute', orientation='h', len=0.33, x=0.1),\n",
    "        showscale=True,\n",
    "        hovertemplate=(\n",
    "            \"Row: %{y}<br>\"  # Display y-coordinate as row index\n",
    "            \"Col: %{x}<br>\"  # Display x-coordinate as column index\n",
    "            \"Spatial: %{customdata[0]}<br>\"  # Value from df1 based on x,y\n",
    "            \"Temp: %{customdata[2]}<br>\"  # Value from df3 based on x,y\n",
    "            \"Attr: %{z}<br>\"  # Value from df2 based on x,y\n",
    "            \"<extra></extra>\"  # Removes extra trace info from hover\n",
    "        ),\n",
    "        customdata=np.dstack((spatial_score_df_, attr_score_df_, temporal_score_df_)),\n",
    "\n",
    "    )\n",
    "    heatmap2 = go.Heatmap(\n",
    "        z=spatial_score_df_.values,\n",
    "        x=list(spatial_score_df_.columns),\n",
    "        y=list(spatial_score_df_.index),\n",
    "        colorscale=colors[1],\n",
    "        opacity=0.2,zmax=150, zauto=False, zmin=0,\n",
    "        colorbar=dict(\n",
    "            title='Spatial',\n",
    "            x=0.8,  # Position at the bottom center\n",
    "            orientation='h', len=0.33\n",
    "        ),\n",
    "        showscale=True\n",
    "    )\n",
    "\n",
    "    # Create the heatmap\n",
    "    heatmap3 = go.Heatmap(\n",
    "        z=temporal_score_df_.values,\n",
    "        x=list(temporal_score_df_.columns),\n",
    "        y=list(temporal_score_df_.index),\n",
    "        opacity=0.2,zmax=150, zauto=False, zmin=0,\n",
    "        colorscale=colors[2],\n",
    "        colorbar=dict(\n",
    "            title='Temporal',\n",
    "            titlefont=dict(size=12),\n",
    "            tickfont=dict(size=10),\n",
    "            x=0.46,  # Position at the bottom center\n",
    "            orientation='h', len=0.33\n",
    "        ),\n",
    "        showscale=True\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=[heatmap3, heatmap2, heatmap1])\n",
    "    fig.update_layout(\n",
    "        width=1800,  # Increase width\n",
    "        height=1800,  # Increase height\n",
    "        xaxis=dict(tickangle=90, tickfont=dict(size=4)),\n",
    "        yaxis=dict(tickfont=dict(size=4)),\n",
    "        title=dict(font=dict(size=20)),\n",
    "        font=dict(size=10)\n",
    "\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "app.run_server(debug=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8eba7632bf44343"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
