{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# QA/QC\n",
    " \n",
    "### What is QA/QC:\n",
    "The task of annotating the quality of collected data/observation:\n",
    "\n",
    "    - GOOD\n",
    "    - BAD\n",
    "    - SUSPECT\n",
    "    - UNKNOWN\n",
    "    \n",
    "### Why QA/QC is needed:\n",
    "\n",
    "Due to different conditions in the natural environment, observations collected by sensors may not be reliable.  \n",
    "The quality assurance and control of data collected from sensor is very important to make sure the data usability.\n",
    "\n",
    "### What are methods to annotate data with QA/QC flags:\n",
    "\n",
    "[IOOS](https://ioos.github.io/ioos_qc/resources.html) has defined standard and statistical methods to annotate quality check on ocean time-series data. As for each essential ocean variable (EoV), different set of statistical tests are recommended to ensure the quality of collected data. Please read the [mannual](https://github.com/ioos/ioos_qc/blob/main/resources/argo-quality-control-manual.pdf) and [mannual In-situ](https://cdn.ioos.noaa.gov/media/2019/08/QARTOD_Currents_Update_Second_Final.pdf)\n",
    "\n",
    "\n",
    "### Why thresholds are necessary\n",
    "\n",
    "[IOOS](https://ioos.github.io/ioos_qc/resources.html) developed a IOOS_QC [QARTOD](https://ioos.github.io/ioos_qc/examples/Qartod_Single_Test_Example.html) python package in which different statistical functions are implemented. As each statistical function requires series to observations (time-series data), as well as it requires additional parameters which are referred as thresholds. \n",
    "\n",
    "For Example:\n",
    "\n",
    "The most basic test (QARTOD function) for each EoV is `range test` where the natural range of value is used to validate the observation (data value). E.g., for sea surface temperature the global range is between -2.5 and 40.0. In this example, the lowest recorded temperature is -2.5 and the highest recorded temperature is 40.0. However, it is uncommon that temperature reaches to extreme that is why `range test` function also takes suspect threshold value to define which values can be suspect to error or warning. \n",
    "\n",
    "### What current datasets are avaiable with QA/QC annotations\n",
    "\n",
    "In CIOOS Atlantic data repository, CMAR is the only partners whose dataset are annotated with QA/QC flags. Here is the list of dataset by CMAR:\n",
    "\n",
    "1. Annapolis County\n",
    "2. Antigonish County\n",
    "3. Cape Breton County\n",
    "4. Colchester County\n",
    "5. Digby County\n",
    "6. Inverness County\n",
    "7. Lunenburg County\n",
    "8. Pictou County\n",
    "9. Queens County\n",
    "10. Richmond County\n",
    "11. Shelburne County\n",
    "12. Victoria County\n",
    "13. Yarmouth County\n",
    "\n",
    "The description related to [CMAR QC Test & Threshold](https://dempsey-cmar.github.io/cmp-data-governance/pages/qc_tests.html#fn1) are provided. The EoVs annotated in these datasets are:\n",
    "\n",
    "- dissolved oxygen\n",
    "- sea surface temperature\n",
    "- salinity\n",
    "- depth check\n",
    "\n",
    "\n",
    "### What we are trying to estimate\n",
    "\n",
    "With the help of domain expert, CMAR has defined [thresholds](res/2024-10-24_cmar_water_quality_thresholds.csv) for each EoVs with respect to QARTOD function and month. Our objective is to learn the thresholds from existing data to annotate QA/QC flags. Estimating and learning threshold from known data will help us to predict QA/QC flags for unknown data which is not annotated.  \n",
    "\n",
    "In this empirical study, we are focusing on `sea surface temperature`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4e4c62e054a58af"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step#1:  Download the data as csv\n",
    "The csv file contain first column as field name and second column contains the unit of each value.\n",
    "\n",
    "## Step#2: Replacing String flag into int Flags\n",
    "By default, the raw data contains flag in String but for easy to process we replace string into integer to optimize. We save the a new file as .csv"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "719e7b4e683cfdd1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "class QartodFlags:\n",
    "    \"\"\"Primary flags for QARTOD.\"\"\"\n",
    "    GOOD = 1\n",
    "    UNKNOWN = 2\n",
    "    SUSPECT = 3\n",
    "    FAIL = 4\n",
    "    MISSING = 9\n",
    "\n",
    "\n",
    "# TARGET EoV is SEA SURFACE TEMPERATURE\n",
    "eov_range = (-2.0, 40)\n",
    "eov_col_name = 'temperature'\n",
    "eov_flag_name = 'qc_flag_temperature'\n",
    "window_hour = 12\n",
    "min_rows_in_a_chunk = 6\n",
    "minimum_rows_for_each_group = 50\n",
    "\n",
    "\n",
    "################# REPLACE FLAGS FROM STRING TO INT FROM CSV##################\n",
    "def custom_replacement(value):\n",
    "    if value == 'Not Evaluated':\n",
    "        return QartodFlags.UNKNOWN\n",
    "    elif value == 'Pass':\n",
    "        return QartodFlags.GOOD\n",
    "    elif value == 'Suspect/Of Interest':\n",
    "        return QartodFlags.SUSPECT\n",
    "    elif value == 'Fail':\n",
    "        return QartodFlags.FAIL\n",
    "    elif math.isnan(float(value)):\n",
    "        return -1\n",
    "    else:\n",
    "        print(f\"Unknown [{value}]\")\n",
    "\n",
    "    return value\n",
    "\n",
    "csv_name = \"D://CIOOS-Full-Data/Annapolis County Water Quality Data.csv\"\n",
    "# if the file is big then process into chunks\n",
    "df_chunks = pd.read_csv(csv_name, chunksize=10000)\n",
    "columns_ = None\n",
    "header_written = False\n",
    "for df in df_chunks:\n",
    "    if columns_ is None:\n",
    "        lst_col  = list(df.columns)\n",
    "        # columns which starts with `qc_` are flag columns\n",
    "        columns_ = [col for col in lst_col if \"qc\" in col.lower()]\n",
    "    for col in columns_:\n",
    "        df[col] = df[col].apply(custom_replacement)\n",
    "\n",
    "    df.to_csv(csv_name.replace(\".csv\", \"_FlagCode.csv\") , index=False, mode='a', header= not header_written)\n",
    "    header_written = True\n",
    "\n",
    "# output: Annapolis County Water Quality Data_FlagCode.csv\n",
    "#######################################################"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e862cf9115ec6012"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step#3: Grouping the data\n",
    "The data is collected on different stations. And each station may have installed one or more sensors. Thus we need to group data by station and sensor. \n",
    "\n",
    "In this code below, the csv `dataframe` is grouped on `station` and `sensor`. Within the data collected, we also need to validate if the collected data is from the same geographical area. That is the reason we group the location with difference of `eps=0.001`.  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b65471eddff931ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def group_with_dbscan(values, eps=0.001): \n",
    "    # Convert to 2D array as required by DBSCAN\n",
    "    X = np.array(values).reshape(-1, 1)\n",
    "    db = DBSCAN(eps=eps, min_samples=1)\n",
    "    labels = db.fit_predict(X)\n",
    "    # Group values by cluster label\n",
    "    groups = []\n",
    "    for label in sorted(set(labels)):\n",
    "        group = [v for v, l in zip(values, labels) if l == label]\n",
    "        groups.append(group)\n",
    "    return groups\n",
    "\n",
    "\n",
    "csv_name = \"D://CIOOS-Full-Data/Annapolis County Water Quality Data_FlagCode.csv\"\n",
    "# lets make subdirectory\n",
    "dir__ = os.path.dirname(csv_name) # D://CIOOS-Full-Data/\n",
    "filename_ = os.path.basename(csv_name) #Annapolis County Water Quality Data_FlagCode.csv\n",
    "new_directory_name = filename_.split(\" \")[0]  #Annapolis\n",
    "save_dir_ = os.path.join(dir__, new_directory_name)\n",
    "os.makedirs(save_dir_) # D://CIOOS-Full-Data/Annapolis/\n",
    "save_name =  os.path.join( save_dir_, filename_.replace(\"_FlagCode.csv\",\"\")) \n",
    "\n",
    "\n",
    "df_ = pd.read_csv(csv_name, parse_dates=['time'], skiprows=[1])\n",
    "df_['latitude'] = df_['latitude'].astype(np.float32).round(4)\n",
    "df_['longitude'] = df_['longitude'].astype(np.float32).round(4)\n",
    "\n",
    "groups_ = df_.groupby(by=['station', 'sensor_serial_number'])\n",
    "id_ = 1\n",
    "for grp_, chunk in groups_:\n",
    "    if chunk.shape[0] < minimum_rows_for_each_group: \n",
    "        continue\n",
    "    # check if the data of more than 1 day is collected\n",
    "    d_threshold = (pd.to_datetime(chunk['time'].max()) - pd.to_datetime(chunk['time'].min())).days > 1\n",
    "    if not d_threshold:\n",
    "        continue\n",
    "\n",
    "    # checking if the data from same sensor is collected from different geographical area\n",
    "    lat_uni_ = chunk['latitude'].unique()\n",
    "    lon_uni_ = chunk['longitude'].unique()\n",
    "    a_ = lat_uni_.std()\n",
    "    b_ = lon_uni_.std()\n",
    "    if a_ > 0.001:\n",
    "        rows__ = chunk.shape[0]\n",
    "        total_rows__ = 0\n",
    "        lat_groups_ = group_with_dbscan(values=lat_uni_)\n",
    "        for j, lat_grp in enumerate(lat_groups_):\n",
    "            sub_chunk_ = chunk[ (chunk['latitude'] >= min(lat_grp)) &  (chunk['latitude'] <= max(lat_grp)) ]\n",
    "            sub_b_ = sub_chunk_['longitude'].astype(np.float32).unique()\n",
    "            assert sub_b_.std() <= 0.001, f\"many longitudes {lat_uni_}\"\n",
    "            sub_chunk_.to_csv(save_name + f\"-{id_}-{j}.csv\", index=False)\n",
    "            total_rows__ += sub_chunk_.shape[0]\n",
    "            print(save_name + f\"-{id_}-{j}.csv\")\n",
    "        assert total_rows__ == rows__, f\"LAT sub chunk rows not equal to main chunk [{lat_groups_}] - [{lat_uni_}]\"\n",
    "\n",
    "    else:\n",
    "        if b_ > 0.001:\n",
    "            rows__ = chunk.shape[0]\n",
    "            total_rows__ = 0\n",
    "            lon_groups_ = group_with_dbscan(values=lon_uni_)\n",
    "            for j, lon_grp in enumerate(lon_groups_):\n",
    "                sub_chunk_ = chunk[(chunk['longitude'] >= min(lon_grp)) & (chunk['longitude'] <= max(lon_grp))]\n",
    "                sub_chunk_.to_csv(save_name + f\"-{id_}-{j}.csv\", index=False)\n",
    "                print(save_name + f\"-{id_}-{j}.csv\")\n",
    "                total_rows__ += sub_chunk_.shape[0]\n",
    "\n",
    "            assert total_rows__ == rows__, f\"LON sub chunk rows not equal to main chunk [{lon_groups_}] - [{lon_groups_}]\"\n",
    "        else:\n",
    "            chunk.to_csv(save_name+f\"-{id_}.csv\", index=False)\n",
    "    id_+=1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "712e5c17f473468b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step#4: Preparing data for Machine learning model\n",
    "\n",
    "\n",
    "\n",
    "![Time-window](res/final_report_diagram.jpg)\n",
    "\n",
    "## Current Feature Set:\n",
    "1)\tRolling Standard Deviation\n",
    "2)\tPast-window mean – current value\n",
    "3)\tFuture-window mean – current value\n",
    "4)\tFuture-window mean – Past-window mean\n",
    "5)\tcurrent value – ( (lead value – lag value)  / 2)\n",
    "6)\tMonth Average Hourly Change – (lag Value – current Value)\n",
    "7)\tMonth Average Hourly Change – (lead Value – current Value)\n",
    "8)\t(Current Value – q_997) if current value > q_997 else 0\n",
    "9)\t(Current Value – q_003) if current value < q_003 else 0\n",
    "10)\t (Current Value – fwq_997) if current value > fwq_997 else 0\n",
    "11)\t(Current Value – fwq_003) if current value > fwq_003 else 0\n",
    "12)\tMonth (1 - 12)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2676faf95170e41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "def apply_rolling_sd_with_time_reset(\n",
    "        df,\n",
    "        time_col='time',\n",
    "        value_col='value',\n",
    "        group_col='group',\n",
    "        time_gap_threshold_min=15,\n",
    "        rolling_window_minutes=60\n",
    "):\n",
    "    \"\"\"\n",
    "    This is implementation of rolling standard deviation defined in [https://dempsey-cmar.github.io/cmp-data-governance/pages/qc_tests.html#fn1]\n",
    "    :param df:  Dataframe \n",
    "    :param time_col: <str> column name of time\n",
    "    :param value_col: <str> column name of value\n",
    "    :param group_col: <str> column name of group\n",
    "    :param time_gap_threshold_min: <int> time gap threshold between two points\n",
    "    :param rolling_window_minutes: <int>\n",
    "    :return: updated dataframe\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[time_col] = pd.to_datetime(df[time_col])\n",
    "    min_wind = int(rolling_window_minutes / time_gap_threshold_min)\n",
    "    # Sort by group and time\n",
    "    df.sort_values([time_col], inplace=True)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Process each group separately\n",
    "    for _, group_df in df.groupby(group_col):\n",
    "        group_df = group_df.copy()\n",
    "\n",
    "        # Calculate time difference in minutes\n",
    "        group_df['time_diff_min'] = group_df[time_col].diff().dt.total_seconds() / 60\n",
    "\n",
    "        # Start a new \"segment\" whenever time gap is too big\n",
    "        group_df['segment'] = (group_df['time_diff_min'] > time_gap_threshold_min).cumsum()\n",
    "\n",
    "        # Within each segment, apply rolling std\n",
    "        def compute_rolling(g):\n",
    "            g = g.set_index(time_col).sort_index()\n",
    "            rolling_window = f'{rolling_window_minutes}min'\n",
    "            g[f'{value_col}_rolling_sd'] = (\n",
    "                g[value_col]\n",
    "                .rolling(rolling_window, center=True, min_periods=min_wind)\n",
    "                .std()\n",
    "            )\n",
    "            g = g.reset_index()\n",
    "            start_ = g.iloc[0][time_col]\n",
    "            for j in range(1, len(g[time_col])):\n",
    "                prev_point_ = g.iloc[j][time_col]\n",
    "                mints = (prev_point_ - start_).seconds / 60\n",
    "                if not (mints < (rolling_window_minutes/2)):\n",
    "                    g.loc[0:j-1 , f'{value_col}_rolling_sd'] = np.nan\n",
    "                    break\n",
    "            start_ = g.iloc[-1][time_col]\n",
    "            for j in range(len(g[time_col])-1, 0, -1):\n",
    "                prev_point_ = g.iloc[j][time_col]\n",
    "                mints = (start_ - prev_point_).seconds / 60\n",
    "                if not (mints < (rolling_window_minutes / 2)):\n",
    "                    g.loc[j+1: , f'{value_col}_rolling_sd'] = np.nan\n",
    "                    break\n",
    "            return g\n",
    "\n",
    "        rolled = group_df.groupby('segment', group_keys=False).apply(compute_rolling, include_groups=False)\n",
    "        results.append(rolled)\n",
    "\n",
    "    # Combine all grouped results\n",
    "    final_df = pd.concat(results, ignore_index=True).drop(columns=['time_diff_min'])\n",
    "    return final_df\n",
    "\n",
    "def generate_time_windows(df, time_col='time', window_hours=2, min_rows_in_chunk=10, filter_flag = None, filter_flag_col = None):\n",
    "    \"\"\"\n",
    "    Rolling over dataframe, generating defined slices based on time-window with step-size = 1\n",
    "    :param df: Dataframe\n",
    "    :param time_col:   Timestamp column name\n",
    "    :param window_hours:  window size in hours\n",
    "    :param min_rows_in_chunk:  minimum rows in a chunk/slice/sequence\n",
    "    :return: list of Tuple( current row, past_time-window-dataframe, previous_time-window-dataframe)\n",
    "    \"\"\"\n",
    "    df = df.dropna(axis=0).sort_values(by=time_col).reset_index(drop=True)\n",
    "    df['time_diff_sec'] = df['time'].diff().dt.total_seconds()\n",
    "    time_delta = timedelta(hours=window_hours, minutes=10)\n",
    "    windowed_samples = []\n",
    "    minimum_seconds_ = window_hours * 60* 59\n",
    "\n",
    "    for i in range(1, len(df)):\n",
    "        current_row = df.iloc[i]\n",
    "        if (filter_flag is not None) and (filter_flag_col is not None):\n",
    "            if current_row[filter_flag_col] != filter_flag:\n",
    "                continue\n",
    "\n",
    "        # OPTIMIZED ###########\n",
    "        future_stop_, past_start_ = i + 1, i\n",
    "        time_sum_past, time_sum_ = 0, 0\n",
    "        past_stop, future_stop = False, False\n",
    "        past_not_satisfied, future_not_satisfied = False, False\n",
    "        while True:\n",
    "            if not (time_sum_ > minimum_seconds_):\n",
    "                if future_stop_ >= len(df):\n",
    "                    future_not_satisfied = True\n",
    "                else:\n",
    "                    next_row = df.iloc[future_stop_]\n",
    "                    next_row_time_diff = next_row['time_diff_sec']\n",
    "                    time_sum_ = time_sum_ + next_row_time_diff\n",
    "                    future_stop_ += 1\n",
    "            else:\n",
    "                future_stop = True\n",
    "\n",
    "            if not (time_sum_past > minimum_seconds_):\n",
    "                if past_start_ < 0:\n",
    "                    past_not_satisfied = True\n",
    "                else:\n",
    "                    prev_row_time_diff = current_row['time_diff_sec']\n",
    "                    time_sum_past = time_sum_past + prev_row_time_diff\n",
    "                    past_start_ -= 1\n",
    "            else:\n",
    "                past_stop = True\n",
    "\n",
    "            if future_not_satisfied or past_not_satisfied:\n",
    "                break\n",
    "            if future_stop and past_stop:\n",
    "                break\n",
    "\n",
    "        if (not past_not_satisfied) and (not future_not_satisfied):\n",
    "            past_slot_df = df.iloc[past_start_:i]\n",
    "            future_slot_df = df.iloc[i + 1: future_stop_]\n",
    "            if (len(past_slot_df) >= min_rows_in_chunk) and ( len(future_slot_df) >= min_rows_in_chunk):\n",
    "                tpl_ = (current_row, past_slot_df.drop_duplicates().reset_index(drop=True),\n",
    "                        future_slot_df.drop_duplicates().reset_index(drop=True))\n",
    "                windowed_samples.append(tpl_)\n",
    "\n",
    "    return windowed_samples\n",
    "\n",
    "\n",
    "def get_xy_from_dataframe(df,eov_flag_name,eov_col_name, window_hours, min_rows_in_chunk=6, ignore_labels = [QartodFlags.UNKNOWN, -1], dataset_name_string = None):\n",
    "    \"\"\"\n",
    "    applying different function in dataframe \n",
    "    :param df: dataframe\n",
    "    :param eov_flag_name: <str> column name wrt to flag_name \n",
    "    :param eov_col_name: <str>\n",
    "    :param window_hours: \n",
    "    :param min_rows_in_chunk: \n",
    "    :param ignore_labels: ignoring these labels for data preparation\n",
    "    :param dataset_name_string: used for print statement \n",
    "    :return: Numpy array \n",
    "    \"\"\"\n",
    "    dt_ = df['time'].dt\n",
    "    df['yr'] = dt_.year\n",
    "    df['mon'] = dt_.month\n",
    "    year_month_stats_map = {}  # (year, month) stats\n",
    "    group_by = df.groupby(by=['yr', 'mon'])\n",
    "    for grp, chunk in group_by:\n",
    "        values_ = chunk[eov_col_name]\n",
    "        q_997 = values_.quantile(0.997)\n",
    "        q_003 = values_.quantile(0.003)\n",
    "        q_mean = values_.mean()\n",
    "        q_std = values_.std()\n",
    "        new_chunk = chunk.copy()\n",
    "        new_chunk = new_chunk.set_index('time')\n",
    "        df_hourly = new_chunk[eov_col_name].resample('1h').mean()\n",
    "        avg_hourly_change = abs(df_hourly.diff()).mean()\n",
    "        year_month_stats_map[grp] = (q_997, q_003, q_mean, q_std, avg_hourly_change)\n",
    "    df['group'] = 1\n",
    "    df = apply_rolling_sd_with_time_reset(\n",
    "        df,\n",
    "        time_col='time',\n",
    "        value_col=eov_col_name,\n",
    "        group_col='group',\n",
    "        time_gap_threshold_min=2 * 60,\n",
    "        rolling_window_minutes= 2 * window_hours * 60\n",
    "    )\n",
    "    df.drop(columns=['group'], inplace=True)\n",
    "    lst_of_seq_ = generate_time_windows(df, window_hours=window_hours, min_rows_in_chunk=min_rows_in_chunk)\n",
    "    # Feature engineering from window\n",
    "    X, y = [], []\n",
    "    for current_, past_, future_ in lst_of_seq_:\n",
    "        label = current_[eov_flag_name]\n",
    "        current_value = current_[eov_col_name]\n",
    "        current_rolling = current_[eov_col_name+\"_rolling_sd\"]\n",
    "\n",
    "        if label in ignore_labels:\n",
    "            continue\n",
    "        past_window = past_[eov_col_name]\n",
    "        future_window = future_[eov_col_name]\n",
    "        if label == QartodFlags.GOOD: #ignoring bad samples in past\n",
    "            Y = past_[eov_flag_name]\n",
    "            mask = ~((Y == QartodFlags.SUSPECT) | (Y == QartodFlags.FAIL))\n",
    "            past_window = past_window[mask]\n",
    "\n",
    "\n",
    "        if (past_window.shape[0] < min_rows_in_chunk) or (future_window.shape[0] < min_rows_in_chunk):\n",
    "            continue\n",
    "\n",
    "\n",
    "        # past_curent_window = pd.concat([past_window, pd.Series([current_value])])\n",
    "        # future_cuurent_window = pd.concat([pd.Series([current_value]), future_window])\n",
    "        full_window = pd.concat([past_window, pd.Series([current_value]), future_window])\n",
    "\n",
    "        q_997, q_003, q_mean, q_std, avg_hourly_change = year_month_stats_map[\n",
    "            (current_['time'].year, current_['time'].month)]\n",
    "        q_997 = abs(current_value - q_997) if current_value > q_997 else 0\n",
    "        q_003 = abs(q_003 - current_value) if current_value < q_003 else 0\n",
    "\n",
    "        fwq_997 = full_window.quantile(0.997)\n",
    "        fwq_003 = full_window.quantile(0.003)\n",
    "        fwq_997 = abs(current_value - fwq_997) if current_value > fwq_997 else 0\n",
    "        fwq_003 = abs(fwq_003 - current_value) if current_value < fwq_003 else 0\n",
    "\n",
    "        # if label == QartodFlags.SUSPECT:\n",
    "        #     a = 10\n",
    "        spike_ref_ = ((past_window.tail(1).values[0] + future_window.head(1).values[0])/2)\n",
    "\n",
    "        month_feature = np.eye(12, dtype=int)[current_['time'].month - 1]\n",
    "        month_feature[current_['time'].month - 1] = 1\n",
    "        past_mean = past_window.mean()\n",
    "        future_mean = future_window.mean()\n",
    "        features = np.array([\n",
    "            current_rolling,\n",
    "            abs(past_mean - current_value),\n",
    "            abs(future_mean - current_value),\n",
    "            abs(future_mean - past_mean),\n",
    "            abs( current_value - spike_ref_ ),\n",
    "            abs(avg_hourly_change - abs(past_window.tail(1).values[0] - current_value)),\n",
    "            abs(avg_hourly_change - abs(future_window.head(1).values[0] - current_value)),\n",
    "            q_997,\n",
    "            q_003,\n",
    "            fwq_997,\n",
    "            fwq_003,\n",
    "            *month_feature,\n",
    "            label\n",
    "        ])\n",
    "\n",
    "        # if np.sum(np.isnan(np.array(features))) > 0:\n",
    "        #     a =10\n",
    "        X.append(features)\n",
    "\n",
    "    X = np.vstack(X)\n",
    "    X = np.round(X, 3)\n",
    "    mask = X[:, -1] == QartodFlags.SUSPECT\n",
    "    suspect_row = X[mask]\n",
    "    mask_f = X[:, -1] == QartodFlags.FAIL\n",
    "    fail_rows = X[mask_f]\n",
    "    mask_g = X[:, -1] == QartodFlags.GOOD\n",
    "    good_rows = X[mask_g]\n",
    "    unique_unique_rows = np.unique(good_rows, axis=0)\n",
    "    print(f\"[{dataset_name_string}] \\n Unique Extraction:  {good_rows.shape}  :-> {unique_unique_rows.shape}  + {fail_rows.shape} + {suspect_row.shape}\")\n",
    "    X = np.concatenate([suspect_row, good_rows, fail_rows])\n",
    "    X = X.astype(np.float32)\n",
    "    X = np.round(X, 3)\n",
    "    return X\n",
    "\n",
    "\n",
    "def get_file_names(dir__):\n",
    "    \"\"\"\n",
    "    :param dir__: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    file_names = []\n",
    "    for root_, dr_, files in os.walk(dir__):\n",
    "        for fl in files:\n",
    "            if fl.lower().endswith(\".csv\"):\n",
    "                file_names.append(os.path.join(root_, fl))\n",
    "    return file_names\n",
    "\n",
    "def controlled_undersample(X, majority_class=QartodFlags.GOOD, reference_class=QartodFlags.SUSPECT, ratio=10):\n",
    "    mask_majority = X[:, -1] == majority_class\n",
    "    mask_reference = X[:, -1] == reference_class\n",
    "    mask_other = X[:, -1] != majority_class\n",
    "\n",
    "    # Subsets\n",
    "    X_majority = X[mask_majority]\n",
    "    X_other = X[mask_other]\n",
    "\n",
    "    # Reference count from class 3\n",
    "    n_reference = np.sum(mask_reference)\n",
    "    n_majority_desired = min(len(X_majority), ratio * n_reference)\n",
    "\n",
    "    # Sample from majority class\n",
    "    indices = np.random.choice(len(X_majority), size=n_majority_desired, replace=False)\n",
    "    X_majority_sampled = X_majority[indices]\n",
    "\n",
    "    # Combine\n",
    "    X_final = np.concatenate([X_other, X_majority_sampled], axis=0)\n",
    "    print(f\"ALL / Controlled Sampling : {X.shape} --> {X_final.shape}\")\n",
    "    return X_final\n",
    "\n",
    "def parl_func(file_name):\n",
    "    print(f\"Processing : [{file_name}]\")\n",
    "    df = pd.read_csv(file_name, usecols=['time', eov_flag_name, eov_col_name])\n",
    "    df.dropna(inplace=True, axis=0)\n",
    "    if df.shape[0] < (min_rows_in_a_chunk*2):\n",
    "        return None\n",
    "\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "    # Feature engineering from window\n",
    "    X1= get_xy_from_dataframe(df, window_hours=window_hour, min_rows_in_chunk=min_rows_in_a_chunk, eov_col_name=eov_col_name,\n",
    "                                   eov_flag_name=eov_flag_name , dataset_name_string=file_name)\n",
    "    if np.sum(np.isnan(X1)) > 0:\n",
    "        print(f\" ---> DATASET WITH NAN: {file_name}\")\n",
    "\n",
    "    return X1\n",
    "\n",
    "def remove_duplicates_from_nparray(nparr_, target_flag):\n",
    "    \"\"\"\n",
    "    last column is the label/flag column in nparr_\n",
    "    :param nparr_:\n",
    "    :param flag:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    mask_g = nparr_[:, -1] == target_flag\n",
    "    good_rows = nparr_[mask_g]\n",
    "    mask = nparr_[:, -1] != target_flag\n",
    "    other_rows_ = nparr_[mask]\n",
    "\n",
    "    unique_unique_rows = np.unique(good_rows, axis=0)\n",
    "    X = np.concatenate([unique_unique_rows, other_rows_])\n",
    "    return X"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7571b5c1f525197"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "chunk_dir = \"D://CIOOS-Full-Data/\"\n",
    "datasets__ = [ \"Annapolis\"]\n",
    "\n",
    "for dataset_ in datasets__:\n",
    "    file_names = get_file_names(os.path.join(chunk_dir, dataset_))  \n",
    "    x_np_array_file = os.path.join(chunk_dir, dataset_, f\"{dataset_}-X_np_array.pkl\")\n",
    "    if os.path.exists(x_np_array_file):\n",
    "        print(f\"-- Skipping  - [{dataset_}] because NPARRAY file exist\")\n",
    "        continue\n",
    "    print(f\"-- Looking Into - [{dataset_}]\")\n",
    "\n",
    "\n",
    "    F_A = None\n",
    "    for fname in tqdm(file_names):   # Uncomment for SEQUETIAL EXECUTION\n",
    "        X1 = parl_func(fname)       # Uncomment for SEQUETIAL EXECUTION\n",
    "    # lst_of_xy = p_umap(parl_func, file_names)   # Uncomment for PARALLEL EXECUTION\n",
    "    # for X1 in lst_of_xy:                        # Uncomment for PARALLEL EXECUTION\n",
    "        if X1 is None:\n",
    "            continue\n",
    "        if F_A is None:\n",
    "            F_A = X1\n",
    "        else:\n",
    "            F_A = np.concatenate((F_A, X1), axis=0)\n",
    "\n",
    "    X = controlled_undersample(F_A, majority_class=QartodFlags.GOOD, reference_class=QartodFlags.SUSPECT, ratio=10)\n",
    "    t_ = F_A.shape\n",
    "    F_A = remove_duplicates_from_nparray(F_A, QartodFlags.GOOD)\n",
    "\n",
    "    X_ = F_A[:, :-1]\n",
    "    Y_ = F_A[:, -1]\n",
    "    print(f\"{t_}  --->  {F_A.shape} ->   {Y_.shape}  ,  {X_.shape}   ===>  .PKL \")\n",
    "\n",
    "    pickle.dump(X_, open(x_np_array_file, 'wb'))\n",
    "    pickle.dump(Y_, open(os.path.join(chunk_dir, dataset_,f\"{dataset_}-Y_np_array.pkl\"), 'wb'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e60a24286894cc6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
